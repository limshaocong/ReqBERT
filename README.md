# ReqBERT: Language Models for Requirements Engineering

## Abstract

Modern engineered systems are immensely complex. The development of such systems is guided by extensive sets of natural language requirements. As such, tools to assist system engineers in managing and extracting information from these requirements must also scale to match the complexity of these systems. However, the systems engineering community have been a laggard in adopting of advanced natural language processing techniques. Pre-trained language models, such as BERT, represent the state-of-the-art in the field. This thesis seeks to understand if these pre-trained language models can achieve higher model performance at a lower computational and manpower cost than earlier techniques. The results show that adapting these language models through task-adaptive pre-training leads to consistent improvements in model performance and greater model robustness. This indicates the potential of applying such language models in the systems engineering domain. However, much work remains to improve model performance and expand possible applications.
